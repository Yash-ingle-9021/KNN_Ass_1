{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344bfc74-4b28-4afc-9c00-a24f1f0aac2d",
   "metadata": {},
   "source": [
    "# KNN Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0330917-118d-4dd1-ba06-c4d67f59c4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48ec3462-cb78-4174-b3d4-4dec1c72e4e1",
   "metadata": {},
   "source": [
    "Q 1 ANS:-\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple and widely used supervised machine learning algorithm for classification and regression tasks. It is a non-parametric method, meaning it makes no assumptions about the underlying data distribution.\n",
    "\n",
    "In KNN, the \"K\" represents the number of nearest neighbors that are considered when making predictions for a new data point. The algorithm works as follows:\n",
    "\n",
    "1. Training Phase: The algorithm memorizes the feature vectors and corresponding labels of the training set.\n",
    "\n",
    "2. Prediction Phase:\n",
    "   - Given a new, unlabeled data point, the algorithm calculates the distance between this point and all the points in the training set. Common distance metrics used include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "   - The algorithm selects the K nearest neighbors (data points with the smallest distances) based on the chosen distance metric.\n",
    "   - For classification tasks, the algorithm assigns the label to the new data point based on the majority class among its K nearest neighbors. The label assignment can be determined through a voting mechanism (e.g., majority vote).\n",
    "   - For regression tasks, the algorithm assigns the output value to the new data point based on the average (or median) value of the output values of its K nearest neighbors.\n",
    "\n",
    "The choice of K is an important hyperparameter in the KNN algorithm. A smaller value of K can make the model more sensitive to noise in the data, potentially leading to overfitting. On the other hand, a larger value of K can make the model more robust to noise but may introduce bias. Selecting an appropriate value of K often requires experimentation and validation.\n",
    "\n",
    "KNN is a relatively simple algorithm, and its main advantages include simplicity, no training time, and interpretability. However, it can be computationally expensive, especially when dealing with large datasets, and it doesn't perform well when the feature space is high-dimensional.\n",
    "\n",
    "It's worth noting that there are variations of KNN, such as weighted KNN, which assigns weights to the neighbors based on their distances, giving more influence to closer neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d94f5-1b8f-432b-bd6a-816de3b3aa58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "981a5e2d-390c-422e-8830-8ff3a11d5624",
   "metadata": {},
   "source": [
    "Q 2 ANS:-\n",
    "\n",
    "Choosing the value of K in KNN is an important consideration and can impact the performance of the algorithm. There is no definitive rule for selecting the optimal K value, but here are a few common approaches:\n",
    "\n",
    "1. Domain Knowledge: It's often helpful to have some domain knowledge about the problem you are working on. Understanding the nature of the data and the characteristics of the classes can provide insights into an appropriate range for K. For example, if the problem involves distinguishing between two classes and the data tends to have clear boundaries, a small value of K (e.g., 3 or 5) may be suitable. On the other hand, if the data is noisy or the decision boundaries are more complex, a larger value of K may be better.\n",
    "\n",
    "2. Cross-Validation: Cross-validation is a technique used to estimate the performance of a model on unseen data. One approach is to perform k-fold cross-validation, where you split the training data into k subsets (folds), train the model on k-1 folds, and evaluate its performance on the remaining fold. You can repeat this process for different values of K and select the one that yields the best average performance across the folds.\n",
    "\n",
    "3. Grid Search: Another method is to perform a grid search over a range of K values. You can define a set of candidate K values, train the KNN model for each value, and evaluate its performance on a validation set. By comparing the results, you can select the K value that achieves the best performance.\n",
    "\n",
    "4. Rule of Thumb: There are some general guidelines that can be used as a starting point. For example, the square root of the number of samples in the training set is often suggested as a reasonable value for K. However, this is not a definitive rule, and the optimal value may vary depending on the specific problem and dataset.\n",
    "\n",
    "It's important to note that the choice of K should be made with careful consideration, and it's recommended to experiment with different values to assess their impact on the model's performance. Additionally, it's worth re-evaluating the choice of K if the dataset or problem characteristics change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce0acf-b4cb-4ad7-85f3-f2ad08a7b6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96a14d63-771e-400d-8100-3782d1b20506",
   "metadata": {},
   "source": [
    "Q 3 ANS:-\n",
    "\n",
    "The difference between KNN classifier and KNN regressor lies in the type of prediction they make and the nature of the target variable they handle.\n",
    "\n",
    "1. KNN Classifier:\n",
    "   - KNN classifier is used for classification tasks, where the goal is to assign a categorical label to each data point.\n",
    "   - The predicted label for a new data point is determined based on the majority class among its K nearest neighbors.\n",
    "   - The output of a KNN classifier is a discrete class label.\n",
    "\n",
    "2. KNN Regressor:\n",
    "   - KNN regressor is used for regression tasks, where the goal is to predict a continuous numeric value for each data point.\n",
    "   - Instead of assigning class labels, KNN regressor calculates the average (or median) value of the output variable for its K nearest neighbors and uses that as the predicted value for the new data point.\n",
    "   - The output of a KNN regressor is a continuous numeric value.\n",
    "\n",
    "In both cases, the KNN algorithm follows a similar process of finding the K nearest neighbors based on a distance metric and making predictions based on those neighbors. The main distinction is in the type of output they generate, which depends on the nature of the target variable.\n",
    "\n",
    "It's worth noting that although KNN can be used for both classification and regression tasks, the performance of the algorithm may vary depending on the specific problem and dataset characteristics. Additionally, there are variations and extensions of KNN specifically designed for regression tasks, such as weighted KNN, where the output values of the neighbors are weighted according to their distances to the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898cbb84-3ac0-4b2d-b69f-9345e711aaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d6ab0b-4aa2-461d-8eb4-3aff41b24d61",
   "metadata": {},
   "source": [
    "Q 4 ANS:-\n",
    "\n",
    "The performance of the KNN algorithm can be evaluated using various performance metrics depending on the specific task, such as classification or regression. Here are some commonly used evaluation measures for KNN:\n",
    "\n",
    "For Classification Tasks:\n",
    "\n",
    "1. Accuracy: This is a widely used metric that calculates the ratio of correctly classified data points to the total number of data points in the evaluation set. It provides an overall measure of the model's predictive accuracy.\n",
    "\n",
    "2. Confusion Matrix: A confusion matrix provides a more detailed view of the classification performance. It presents the number of true positive, true negative, false positive, and false negative predictions, allowing you to calculate other metrics such as precision, recall, and F1-score.\n",
    "\n",
    "3. Precision, Recall, and F1-score: These metrics are useful when the classes are imbalanced or when the cost of false positives and false negatives differs. Precision calculates the proportion of true positives among all predicted positives, while recall calculates the proportion of true positives among all actual positives. F1-score is the harmonic mean of precision and recall, providing a balanced measure of the model's performance.\n",
    "\n",
    "4. ROC Curve and AUC: Receiver Operating Characteristic (ROC) curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various classification thresholds. The Area Under the Curve (AUC) summarizes the performance of the model across different thresholds. It is particularly useful when dealing with imbalanced datasets.\n",
    "\n",
    "For Regression Tasks:\n",
    "\n",
    "1. Mean Squared Error (MSE): This measures the average squared difference between the predicted and actual values. It gives higher weight to larger errors.\n",
    "\n",
    "2. Root Mean Squared Error (RMSE): RMSE is the square root of MSE and is often used to provide a more interpretable measure of the average prediction error.\n",
    "\n",
    "3. Mean Absolute Error (MAE): This calculates the average absolute difference between the predicted and actual values. It provides a measure of the average magnitude of errors.\n",
    "\n",
    "4. R-squared (RÂ²) or Coefficient of Determination: This metric assesses the proportion of the variance in the target variable that is explained by the model. It ranges from 0 to 1, with higher values indicating better model fit.\n",
    "\n",
    "It's important to select evaluation metrics that are appropriate for the specific task and consider the characteristics and requirements of the problem at hand. Additionally, it is common practice to validate the performance of the model using techniques such as cross-validation to assess its generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad928db-4a74-4b45-bbce-f8593aa95874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b62e793b-1df0-4a3d-b7f0-bbdc112e85aa",
   "metadata": {},
   "source": [
    "Q 5 ANS:-\n",
    "\n",
    "The \"curse of dimensionality\" refers to a phenomenon that occurs in high-dimensional spaces and can negatively impact the performance and efficiency of various machine learning algorithms, including KNN.\n",
    "\n",
    "In the context of KNN, the curse of dimensionality arises because the distance-based calculations become less reliable and meaningful as the number of dimensions increases. This has several implications:\n",
    "\n",
    "1. Increased Sparsity: In high-dimensional spaces, data points tend to become more sparse. As the number of dimensions grows, the available data points become sparser, making it more challenging to find meaningful and representative neighbors.\n",
    "\n",
    "2. Increased Irrelevance of Neighbors: In higher dimensions, the distance between data points tends to become more uniform. This means that data points that are \"close\" to each other in terms of distance may not necessarily be similar in terms of their feature values. This can lead to less reliable predictions as the nearest neighbors may not truly represent the underlying patterns.\n",
    "\n",
    "3. Computational Complexity: As the number of dimensions increases, the computational cost of calculating distances between data points grows exponentially. This can make the KNN algorithm computationally expensive and time-consuming, particularly for large datasets.\n",
    "\n",
    "4. Overfitting: In high-dimensional spaces, the risk of overfitting increases. With a large number of dimensions, the model can become excessively sensitive to noise and outliers, which can lead to poor generalization performance.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN and other high-dimensional problems, some techniques and considerations include:\n",
    "\n",
    "- Dimensionality Reduction: Techniques such as Principal Component Analysis (PCA) or t-SNE can be used to reduce the dimensionality of the feature space while preserving important information.\n",
    "\n",
    "- Feature Selection: Choosing relevant and informative features can help reduce the dimensionality and focus on the most discriminative aspects of the data.\n",
    "\n",
    "- Feature Engineering: Transforming or creating new features based on domain knowledge can enhance the discriminatory power and reduce the noise in the data.\n",
    "\n",
    "- Regularization: Applying regularization techniques can help prevent overfitting and improve generalization by reducing the model's complexity.\n",
    "\n",
    "- Consider Other Algorithms: In some cases, alternative algorithms may be more suitable for high-dimensional problems, such as linear models or tree-based methods.\n",
    "\n",
    "Overall, understanding the curse of dimensionality and employing appropriate techniques to address it is crucial when working with high-dimensional data in KNN and other machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a030a65-0841-42b4-aba0-73ce62452ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8feef9d-b371-4bdc-815f-37814c15ed00",
   "metadata": {},
   "source": [
    "Q 6 ANS:-\n",
    "\n",
    "Handling missing values in KNN requires addressing the gaps in the data to ensure accurate distance calculations and meaningful neighbor selection. Here are a few approaches commonly used to handle missing values in KNN:\n",
    "\n",
    "1. Deletion: One simple approach is to remove data points with missing values. However, this can lead to a significant loss of data, especially if the missing values are widespread. It is generally recommended to use this approach sparingly, particularly when missing values are relatively small in number.\n",
    "\n",
    "2. Imputation: Imputation involves estimating missing values based on the available data. Various imputation techniques can be applied, such as:\n",
    "\n",
    "   - Mean or Median Imputation: Replace missing values with the mean or median value of the respective feature across the entire dataset. This approach assumes that the missing values are missing at random and that the overall distribution of the feature is representative.\n",
    "\n",
    "   - Mode Imputation: For categorical features, replace missing values with the mode (most frequent value) of the feature.\n",
    "\n",
    "   - Regression Imputation: Use regression models to predict missing values based on other features. This approach takes into account the relationships between features and can be more accurate if the data exhibits correlations.\n",
    "\n",
    "   - KNN Imputation: In this approach, missing values are estimated based on the values of K nearest neighbors. The K nearest neighbors are selected based on the available features, and their values are used to impute the missing values. This method leverages the similarities between data points to estimate missing values. However, it introduces a recursive aspect, as the KNN algorithm itself relies on complete data. Therefore, it may require iterations to impute missing values in multiple passes.\n",
    "\n",
    "3. Indicator Variables: Instead of imputing missing values directly, you can create additional binary indicator variables to represent the presence or absence of missing values in each feature. This approach allows the KNN algorithm to consider the missingness pattern as a feature in itself.\n",
    "\n",
    "It's essential to note that the choice of the appropriate method for handling missing values depends on the specific dataset, the extent and patterns of missingness, and the underlying assumptions of the data. Additionally, it's important to evaluate the impact of the chosen imputation method on the performance and validity of the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd64281-6060-4372-b2ac-57e8f766ffc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef78219-8e5c-4292-bcfc-378d05c40d67",
   "metadata": {},
   "source": [
    "Q 7 ANS:-\n",
    "\n",
    "The performance of the KNN classifier and regressor can vary depending on the specific problem and dataset characteristics. Here are some points of comparison between the two:\n",
    "\n",
    "1. Prediction Task:\n",
    "   - KNN Classifier: The classifier is used for classification tasks where the goal is to assign categorical labels to data points.\n",
    "   - KNN Regressor: The regressor is used for regression tasks where the goal is to predict continuous numeric values.\n",
    "\n",
    "2. Output Type:\n",
    "   - KNN Classifier: The output of a classifier is a discrete class label. It assigns the data point to a specific category or class.\n",
    "   - KNN Regressor: The output of a regressor is a continuous numeric value. It predicts the value of the target variable for the data point.\n",
    "\n",
    "3. Evaluation Metrics:\n",
    "   - KNN Classifier: The performance of a classifier is typically evaluated using metrics such as accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "   - KNN Regressor: The performance of a regressor is usually evaluated using metrics like mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared (coefficient of determination).\n",
    "\n",
    "4. Handling of Outliers:\n",
    "   - KNN Classifier: The classifier is generally robust to outliers as it focuses on class assignment based on majority voting.\n",
    "   - KNN Regressor: The regressor can be sensitive to outliers since it aims to predict continuous values. Outliers may significantly impact the average or median value calculations for nearest neighbors.\n",
    "\n",
    "5. Dataset Size:\n",
    "   - KNN Classifier: The classifier can handle datasets of various sizes since it only relies on the neighbors within the specified K value.\n",
    "   - KNN Regressor: The regressor's performance may be affected by the dataset size, particularly when the number of data points is limited. Sparse data can result in a less representative set of nearest neighbors.\n",
    "\n",
    "Regarding which one is better for a specific problem, it depends on the nature of the problem and the type of the target variable:\n",
    "- If the problem requires classifying data into discrete categories or classes, the KNN classifier is generally a suitable choice.\n",
    "- If the problem involves predicting continuous numeric values, the KNN regressor is more appropriate.\n",
    "\n",
    "However, the ultimate choice should consider the specific characteristics of the dataset, the assumptions of the algorithms, and potentially experiment with both approaches to determine which yields better performance for the particular problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a807b-ad71-46ed-939b-d56b1d743c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3343f0c-116e-4a93-86e8-dc11e8371c6d",
   "metadata": {},
   "source": [
    "Q 8 ANS:-\n",
    "\n",
    "The KNN algorithm has its strengths and weaknesses for both classification and regression tasks. Let's discuss them and potential ways to address them:\n",
    "\n",
    "Strengths of KNN:\n",
    "\n",
    "1. Intuitive and Simple: KNN is easy to understand and implement, making it accessible for beginners in machine learning.\n",
    "\n",
    "2. No Training Phase: KNN is a lazy learning algorithm, meaning it does not require a training phase. The model simply stores the training data, making it quick to apply on new data.\n",
    "\n",
    "3. Non-Parametric: KNN makes no assumptions about the underlying data distribution, making it suitable for a wide range of problem domains.\n",
    "\n",
    "4. Robust to Noise: KNN can handle noisy data and outliers since it considers the neighbors' votes (for classification) or values (for regression) rather than relying on specific assumptions.\n",
    "\n",
    "Weaknesses of KNN:\n",
    "\n",
    "1. Computational Complexity: Calculating distances between data points can be computationally expensive, especially with large datasets and high-dimensional feature spaces. This can slow down the training and prediction process.\n",
    "\n",
    "2. Sensitivity to Feature Scaling: KNN is sensitive to the scale of features. Features with larger scales can dominate the distance calculations, leading to biased predictions. Scaling the features to a similar range can help address this issue.\n",
    "\n",
    "3. Curse of Dimensionality: As the dimensionality of the feature space increases, the performance of KNN tends to degrade due to the sparsity of data, the increased irrelevance of neighbors, and the computational burden. Dimensionality reduction techniques or feature selection can help mitigate this issue.\n",
    "\n",
    "4. Optimal K Selection: Choosing the appropriate value of K is crucial. A small K may lead to overfitting, while a large K may introduce bias and lead to underfitting. Cross-validation or grid search can help determine the optimal K value.\n",
    "\n",
    "Addressing the Weaknesses:\n",
    "\n",
    "1. Efficient Data Structures: Using efficient data structures like KD-trees or Ball trees can speed up the nearest neighbor search, reducing the computational complexity.\n",
    "\n",
    "2. Feature Scaling: Normalize or standardize the features to ensure they have similar scales. Techniques like min-max scaling or z-score normalization can help overcome the sensitivity to feature scaling.\n",
    "\n",
    "3. Dimensionality Reduction: Apply dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection methods to reduce the number of dimensions and improve the algorithm's performance.\n",
    "\n",
    "4. Model Selection and Tuning: Experiment with different values of K and evaluate the model's performance using cross-validation or validation sets to choose the optimal K value. Additionally, consider using weighted KNN or other variants to address specific challenges or improve performance in certain scenarios.\n",
    "\n",
    "By considering these strengths, weaknesses, and corresponding strategies, the limitations of KNN can be mitigated, and its performance can be improved for classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555fda4-0e4e-440f-bc33-002255c5a6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cc06085-f1ad-481b-829e-2c59883d881c",
   "metadata": {},
   "source": [
    "Q 9 ANS:-\n",
    "\n",
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN and other machine learning algorithms. They differ in how they calculate the distance between two data points in a feature space:\n",
    "\n",
    "1. Euclidean Distance:\n",
    "   - Euclidean distance is a straight-line distance between two points, which corresponds to the length of the shortest path connecting the points in a Cartesian coordinate system.\n",
    "   - Mathematically, the Euclidean distance between two points (p1, q1) and (p2, q2) in a 2D space is calculated as: \n",
    "     d = â((p2 - p1)^2 + (q2 - q1)^2)\n",
    "   - Euclidean distance takes into account both the vertical and horizontal differences between points, resulting in a measure of their overall spatial separation.\n",
    "   - Euclidean distance is sensitive to the magnitude of differences in each dimension.\n",
    "\n",
    "2. Manhattan Distance:\n",
    "   - Manhattan distance, also known as city block distance or L1 norm, measures the distance between two points by summing the absolute differences in their coordinates.\n",
    "   - Mathematically, the Manhattan distance between two points (p1, q1) and (p2, q2) in a 2D space is calculated as:\n",
    "     d = |p2 - p1| + |q2 - q1|\n",
    "   - Manhattan distance represents the distance traveled when moving between two points in a city-like grid, where you can only move horizontally and vertically.\n",
    "   - Unlike Euclidean distance, Manhattan distance only considers the differences along each axis independently and ignores diagonal or straight-line paths.\n",
    "\n",
    "Comparison:\n",
    "- Euclidean distance is more sensitive to the magnitude of differences in each dimension, while Manhattan distance treats all dimensions equally.\n",
    "- Euclidean distance accounts for the overall spatial separation and considers diagonal paths, while Manhattan distance only considers vertical and horizontal paths.\n",
    "- Euclidean distance is suitable when the underlying relationships between features are continuous and smooth, while Manhattan distance is more appropriate for situations where features are discrete or when movement along axes is restricted.\n",
    "- The choice between Euclidean and Manhattan distance depends on the specific problem, the nature of the features, and the characteristics of the dataset. It is common to experiment with both metrics to determine which one performs better for a given task.\n",
    "\n",
    "It's worth noting that there are other distance metrics available, such as Minkowski distance (which includes both Euclidean and Manhattan distance as special cases) or Mahalanobis distance (which accounts for the correlation between features), that can be used in KNN depending on the problem requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930dcb4-00af-4943-9b30-a82850b162dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43743974-02c8-4cb2-937b-70854a32b9bc",
   "metadata": {},
   "source": [
    "Q 10 ANS:-\n",
    "\n",
    "Feature scaling plays a crucial role in KNN by ensuring that all features contribute equally to the distance calculations. Without proper scaling, features with larger scales can dominate the distance computations, leading to biased results and potentially incorrect neighbor selections. Here are the key roles of feature scaling in KNN:\n",
    "\n",
    "1. Equalizing Feature Influence: Feature scaling brings all features to a similar scale, which prevents one feature from dominating the distance calculations. In KNN, distances between data points are typically calculated using metrics like Euclidean or Manhattan distance. If features have significantly different scales, those with larger values will contribute more to the distance calculations, potentially overshadowing the contributions of other features.\n",
    "\n",
    "2. Addressing Different Measurement Units: In datasets, different features may have different units of measurement. For example, one feature may represent time in seconds, while another feature represents a monetary value in dollars. These differences in measurement units can lead to misleading distance calculations. By scaling the features, the algorithm can treat all variables on a consistent scale, enabling meaningful comparisons.\n",
    "\n",
    "3. Improving Convergence: Feature scaling can improve the convergence of the algorithm during the training phase. Distance-based algorithms like KNN often rely on iterative processes, such as gradient descent or nearest neighbor search. Scaling the features helps the algorithm converge more efficiently by avoiding oscillations or slow convergence caused by disparate scales.\n",
    "\n",
    "4. Handling Outliers: Scaling features can help in handling outliers. Outliers can significantly impact the distance calculations and influence the neighbor selection process in KNN. By scaling the features, the impact of outliers can be mitigated, as the scaled values are brought closer together, making the algorithm more robust to extreme values.\n",
    "\n",
    "Common techniques for feature scaling in KNN include:\n",
    "\n",
    "- Min-Max Scaling (Normalization): This technique scales the features to a specific range, typically between 0 and 1. It preserves the relative relationships between data points but may be sensitive to outliers.\n",
    "\n",
    "- Standardization (Z-Score Scaling): This technique standardizes the features to have zero mean and unit variance. It centers the data around the mean and scales it based on the standard deviation. It is less affected by outliers and works well with features that follow a Gaussian distribution.\n",
    "\n",
    "By applying appropriate feature scaling techniques, KNN can make fair and meaningful distance calculations, ensuring that all features contribute equally to the classification or regression decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b95344-47d1-4a02-9a55-6e70018246e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
